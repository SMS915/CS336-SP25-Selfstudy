import os
import json
import time
from typing import List, Dict, Callable
from vllm import LLM, SamplingParams

# å¼•å…¥è¯„åˆ†å‡½æ•°
from cs336_alignment.drgrpo_grader import r1_zero_reward_fn

def load_data(file_path: str) -> List[Dict]:
    examples = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            examples.append(json.loads(line))
    return examples

def formatting_prompt(examples: List[Dict], prompt_template: str) -> List[str]:
    prompts = []
    for ex in examples:
        # æ›¿æ¢å ä½ç¬¦ï¼Œç¡®ä¿å’Œè®­ç»ƒæ—¶çš„æ ¼å¼ä¸€è‡´
        prompt = prompt_template.replace("{question}", ex["problem"])
        prompts.append(prompt)
    return prompts

def evaluate_sft_model():
    # ================= é…ç½®åŒºåŸŸ =================
    # 1. æŒ‡å‘ä½ è®­ç»ƒä¿å­˜çš„ Checkpoint ç›®å½•
    MODEL_PATH = "checkpoints/sft_v1" 
    
    # 2. æ•°æ®è·¯å¾„
    DATA_PATH = "data/MATH/validation.jsonl"
    PROMPT_PATH = "cs336_alignment/prompts/r1_zero.prompt"
    OUTPUT_FILE = "results/sft_eval_results.jsonl"
    
    # 3. å…³é”®ï¼šç»™å¤Ÿç”Ÿæˆé•¿åº¦ï¼
    MAX_TOKENS = 4096 
    # ===========================================

    print(f"ğŸš€ Loading SFT model from: {MODEL_PATH}")
    
    # 1. åˆå§‹åŒ– vLLM
    # trust_remote_code=True å³ä½¿åŠ è½½æœ¬åœ°æ¨¡å‹æœ‰æ—¶ä¹Ÿéœ€è¦ï¼Œå–å†³äº config
    # æ˜¾å­˜åˆ©ç”¨ç‡è®¾ä¸º 0.9ï¼Œå› ä¸ºç°åœ¨åªè·‘æ¨ç†ï¼Œä¸è®­ç»ƒ
    llm = LLM(
        model=MODEL_PATH,
        dtype="bfloat16",
        gpu_memory_utilization=0.9,
        trust_remote_code=True,
        max_model_len=MAX_TOKENS  # ç¡®ä¿ KV Cache é¢„ç•™å¤Ÿç©ºé—´
    )

    # 2. å‡†å¤‡æ•°æ®
    print("Loading data and prompts...")
    examples = load_data(DATA_PATH)
    
    # å»ºè®®å…ˆè·‘å‰ 50-100 æ¡çœ‹çœ‹æ•ˆæœï¼Œå…¨é‡ 5000 æ¡å¯èƒ½è¦è·‘ä¸€ä¼šå„¿
    # examples = examples[:100] 
    
    with open(PROMPT_PATH, "r") as f:
        prompt_template = f.read()
    
    prompts = formatting_prompt(examples, prompt_template)

    # 3. è®¾ç½®é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=0.6, # SFT åé€šå¸¸å¯ä»¥ç¨å¾®é™ä½ä¸€ç‚¹æ¸©åº¦ï¼Œæˆ–è€…ä¿æŒ 1.0
        top_p=0.95,
        max_tokens=MAX_TOKENS,
        stop=["</answer>"],             # é‡åˆ°æ­¤æ ‡è®°åœæ­¢
        include_stop_str_in_output=True # ä¿ç•™æ ‡è®°
    )

    # 4. ç”Ÿæˆ
    print(f"å¼€å§‹ç”Ÿæˆ{len(prompts)} æ¡æ•°æ®")
    start_time = time.time()
    outputs = llm.generate(prompts, sampling_params)
    end_time = time.time()
    print(f"ç”Ÿæˆå®Œæˆï¼Œå…±ç”¨æ—¶{end_time - start_time:.2f}ç§’")

    # 5. è¯„åˆ†
    results = []
    correct_count = 0
    format_error_count = 0
    ans_error_count = 0
    
    print("ğŸ“Š Scoring...")
    for i, output in enumerate(outputs):
        generated_text = output.outputs[0].text
        example = examples[i]
        truth = example["solution"]
        text_for_grading = generated_text.replace("</think><answer>", "</think> <answer>")
        # è¯„åˆ†
        # æ³¨æ„ï¼šgenerated_text å¼€å¤´å¯èƒ½æ²¡æœ‰ <think> (å› ä¸ºå®ƒåœ¨ prompt é‡Œ)
        # ä½† grader ä¸»è¦çœ‹ <answer>ï¼Œé€šå¸¸æ²¡é—®é¢˜ã€‚
        metrics = r1_zero_reward_fn(text_for_grading, truth)
        if metrics.get("reward", 0.0) == 1.0:
            correct_count += 1
        elif metrics.get("format_reward", 0.0) == 1.0:
            ans_error_count += 1
        else:
            format_error_count += 1

        results.append({
            "problem": example["problem"],
            "gold_solution": truth,
            "generated_text": generated_text,
            "metrics": metrics
        })
    # 6. æ‰“å°æŠ¥å‘Š
    accuracy = correct_count / len(prompts)

    
    print("\n" + "="*30)
    print("è¯„ä¼°ç»“æœå¦‚ä¸‹:")
    print(f"Model: {MODEL_PATH}")
    print(f"å®Œå…¨æ­£ç¡®: {accuracy: .2%}")
    print(f"æ ¼å¼æ­£ç¡®ï¼Œç­”æ¡ˆé”™è¯¯: {ans_error_count / len(prompts):.2%}")
    print(f"æ ¼å¼é”™è¯¯: {format_error_count / len(prompts):.2%}")
    print("="*30 + "\n")

    # 7. ä¿å­˜
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for res in results:
            f.write(json.dumps(res) + "\n")
    print(f" ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_FILE}")

if __name__ == "__main__":
    evaluate_sft_model()