# base_modern.yaml
# ==========================================
# 1. 基础与系统设置 (General & System)
# ==========================================
project_name: "CS336-TransformerLM-OpenWebText-Modern"
seed: 42                                  # 固定随机种子，保证实验可复现
device: "cuda"                            # 训练设备
load_ckpt: False                          # 是否加载旧权重（Resume Training）
save_ckpt: True                           # 是否保存权重
checkpoint_path: "checkpoints/"           # 权重保存路径

# ==========================================
# 2. 数据设置 (Data Configuration)
# ==========================================
train_data_path: "data/owt_train.bin"     # 预处理好的二进制训练数据
val_data_path: "data/owt_valid.bin"       # 验证数据
batch_size: 16                            # 单卡单次前向传播的样本数
context_length: 1024                      # 序列长度 (Sequence Length)
token_dtype: "uint16"                     # 数据存储精度，uint16 可节省内存

# ==========================================
# 3. 模型架构设置 (Model Architecture)
# ==========================================
vocab_size: 50257                         # 词表大小 (GPT-2 tokenizer)
n_layers: 12                              # Transformer 层数
n_heads: 8                                # 注意力头数
d_model: 512                              # 嵌入维度 (Embedding Dimension)
d_ff: null                                # FFN 隐层维度 (null 表示自动计算，通常 SwiGLU 为 8/3 * d_model)
rope_theta: 10000.0                       # RoPE 的波长基数

# 以下为消融的可选实验参数
n_kv_heads: null                          # KV 头数 (null 表示等于 n_heads，即不使用 GQA/MQA)

activation: "silu"                        # 激活函数 ['silu', 'relu', 'gelu' 等]
gated_ffn: True                           # 开启 Gated FFN (即 SwiGLU 结构)
gated_attn: False                         # 开启 Gated Attention
Weight_Tying: True                        # 共享 Input Embedding 和 Output Projection 权重

post_norm: False                          # Post-Norm 结构开关参数
layer_norm: False                         # 是否使用 LayerNorm (若为 False 则使用 RMSNorm)
no_norm: False                            # 完全移除 Norm，观察无Norm下训练情况

pos_emb_type: 'rope'                      # 'rope': 旋转位置编码; 'sinusoidal': 正弦位置编码; 'learned': 嵌入矩阵学习位置编码; 'removed': 无位置编码

bias: False                               # 禁用 Linear 层偏置 (现代 LLM 常见做法)

flash_attn: True                          # Flash-attention启用开关，开启时使用nn.functional内置的 scaled_dot_product_attention，反之使用自己实现的偏低效率SDPA

# ==========================================
# 4. 训练流程设置 (Training Loop)
# ==========================================
max_steps: 10000                          # 总训练步数，以一次梯度累积为单位
warmup_steps: 1000                        # 学习率预热步数
cycle_steps: 7000                         # 调度器周期 (用于 Cosine/WSD decay)
accumulate_size: 8                        # 单次梯度累积步数 (真实batch_size = batch_size * accumulate_size)
grad_checkpointing: False                 # 是否开启梯度检查点

log_interval: 50                          # 控制台打印频率
eval_interval: 200                        # 验证集评估频率
eval_steps: 50                            # 每次评估使用的 Batch 数量
checkpoint_interval: 1000                  # 模型保存频率

# ==========================================
# 5. 优化器与调度器 (Optimizer & Scheduler)
# ==========================================
lr_schedule: 'wsd'                        # 调度器：cosine (余弦退火) 或者 wsd (Warmup-Stable-Decay)
max_learning_rate: 5e-4                   # 峰值学习率
min_learning_rate: 5e-5                   # 衰减终点学习率
weight_decay: 0.1                         # 权重衰减 (通常 0.1 适合 AdamW)
max_grad_norm: 1.0                        # 梯度裁剪阈值

beta1: 0.9                                # AdamW Beta1
beta2: 0.95                               # AdamW Beta2
eps: 1e-8                                 # 数值稳定性项